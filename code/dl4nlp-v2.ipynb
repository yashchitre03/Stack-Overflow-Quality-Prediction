{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Instructions to run:\n1. Install all the libraries given in the imports.\n2. Download the Stack Overflow dataset (2016-2020) from here (https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate).\n3. Open in a console that supports .ipynb or python notebooks.\n4. Run the program on the console and wait for the program to finish."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import *\nimport matplotlib.pyplot as plt\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom plotly import graph_objs as go\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nimport random\nimport numpy as np\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_confusion_matrix(accuracy, labels, val_pred_labels, val_true_labels):\n    '''\n    Plots the confusion matrix for given data.\n    '''\n    confusion_matrix = np.zeros((3, 3), dtype=np.int16)\n    for pred, true in zip(val_pred_labels, val_true_labels):\n        confusion_matrix[pred, true] += 1\n    confusion_matrix = confusion_matrix / confusion_matrix.sum(axis=1,keepdims=1)\n    \n    plt.figure(figsize=(20, 20))\n    sns.set(font_scale=1.5)\n    ax = sns.heatmap(confusion_matrix, annot=True, square=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Scale'})\n    ax.set_title(f'Confusion Matrix (Accuracy = {accuracy})', fontsize=50)\n    ax.set_xticklabels(labels, fontsize=15)\n    ax.set_yticklabels(labels, fontsize=15)\n    ax.set_xlabel(\"True Label\", fontsize=25)\n    ax.set_ylabel(\"Predicted Label\", fontsize=25)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(r'/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv')\ntest_data = pd.read_csv(r'/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv')\ndata = pd.concat((train_data, test_data))\nprint(f'The size of training dataset is: {len(train_data)} samples')\nprint(f'The size of testing dataset is: {len(test_data)} samples')\nprint(f'The combined size of the dataset is: {len(data)} samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Y.value_counts().plot.bar()\nplt.xlabel('Classes')\nplt.ylabel('Number of samples')\nplt.title('Dataset size')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Text']=data.Body.apply(lambda x: BeautifulSoup(x, 'html.parser').text)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HQ = data[data['Y']=='HQ']['Text'].str.split().map(lambda x: len(x) if len(x) < 500 else 500)\nLQ_EDIT = data[data['Y']=='LQ_EDIT']['Text'].str.split().map(lambda x: len(x) if len(x) < 500 else 500)\nLQ_CLOSE = data[data['Y']=='LQ_CLOSE']['Text'].str.split().map(lambda x: len(x) if len(x) < 500 else 500)\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=HQ, histfunc='avg', name=\"HQ\", opacity=0.6, histnorm='probability density'))\nfig.add_trace(go.Histogram(x=LQ_EDIT, histfunc='avg', name=\"LQ_EDIT\", opacity=0.6, histnorm='probability density'))\nfig.add_trace(go.Histogram(x=LQ_CLOSE, histfunc='avg', name=\"LQ_CLOSE\", opacity=0.6, histnorm='probability density'))\n\nfig.update_layout(\n    title_text='Question word count frequency',\n    xaxis_title_text='Word count',\n    yaxis_title_text='Frequency',\n    bargap=0.2,\n    bargroupgap=0.1,\n    barmode='overlay'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 360\nVOCAB_SIZE = 100000\nEPOCHS = 25\nBATCH_SIZE = 32\nOOV_TOKEN = '<UNK>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, remain = train_test_split(data, test_size=0.2, random_state=0)\nvalidation, test = train_test_split(remain, test_size=0.5, random_state=0)\n\nprint(f'The size of training dataset is: {len(train)} samples')\nprint(f'The size of validation dataset is: {len(validation)} samples')\nprint(f'The size of test dataset is: {len(test)} samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(data.Y.values)\n\nencoded_train_Y = encoder.transform(train.Y.values)\nencoded_valid_Y = encoder.transform(validation.Y.values)\nencoded_test_Y = encoder.transform(test.Y.values)\n\ntrain_X = train.Text.values\nvalid_X = validation.Text.values\ntest_X = test.Text.values\n\ntrain_Y = np_utils.to_categorical(encoded_train_Y)\nvalid_Y = np_utils.to_categorical(encoded_valid_Y)\ntest_Y = np_utils.to_categorical(encoded_test_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\ntokens.fit_on_texts(list(train_X))\n\ntrain_X = tokens.texts_to_sequences(train_X)\nvalid_X = tokens.texts_to_sequences(valid_X)\ntest_X = tokens.texts_to_sequences(test_X)\n\nprint(np.quantile([len(x) for x in train_X], 0.95))\n\ntrain_X = preprocessing.sequence.pad_sequences(train_X, maxlen=SEQ_LEN, padding='post', truncating='post')\nvalid_X = preprocessing.sequence.pad_sequences(valid_X, maxlen=SEQ_LEN, padding='post', truncating='post')\ntest_X = preprocessing.sequence.pad_sequences(test_X, maxlen=SEQ_LEN, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(None,), dtype=\"int32\")\n\nx = layers.Embedding(VOCAB_SIZE, 128)(inputs)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64))(x)\n\noutputs = layers.Dense(3, activation=\"softmax\")(x)\n\nmodel = Model(inputs, outputs)\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer=optimizers.Adam(lr=1e-4), \n              metrics=['accuracy'],)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [callbacks.ModelCheckpoint('lstm.h5', monitor='val_loss', save_best_only=True, verbose=2),\n                callbacks.EarlyStopping(monitor='val_loss',  patience=5, verbose=2),\n                callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=2)]\n\nlstm = model.fit(train_X, \n                train_Y, \n                batch_size=BATCH_SIZE, \n                epochs=EPOCHS, \n                validation_data=(valid_X, valid_Y),\n                callbacks = my_callbacks, \n                verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(test_X, test_Y)\n\npred_Y = model.predict(test_X)\npred_Y = np.argmax(pred_Y, axis=1)\n\ntrue_Y = np.argmax(test_Y, axis=1)\n\nCLASSES = list(encoder.classes_)\nprint(classification_report(true_Y, pred_Y, target_names=CLASSES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_confusion_matrix(round(accuracy, 4), CLASSES, pred_Y, true_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(SEQ_LEN,), dtype=\"int32\")\n\nx = layers.Embedding(VOCAB_SIZE, 128)(inputs)\nx = layers.Conv1D(32, 3, padding='same', activation='relu')(x)\nx = layers.MaxPooling1D()(x)\nx = layers.Flatten()(x)\nx = layers.Dense(128, activation='relu')(x)\n\noutputs = layers.Dense(3, activation=\"softmax\")(x)\n\nmodel = Model(inputs, outputs)\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer=optimizers.Adam(lr=1e-4), \n              metrics=['accuracy'],)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [callbacks.ModelCheckpoint('cnn.h5', monitor='val_loss', save_best_only=True, verbose=2),\n                callbacks.EarlyStopping(monitor='val_loss',  patience=5, verbose=2),\n                callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=2)]\n\ncnn = model.fit(train_X, \n                train_Y, \n                batch_size=BATCH_SIZE, \n                epochs=EPOCHS, \n                validation_data=(valid_X, valid_Y),\n                callbacks = my_callbacks, \n                verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(test_X, test_Y)\n\npred_Y = model.predict(test_X)\npred_Y = np.argmax(pred_Y, axis=1)\n\ntrue_Y = np.argmax(test_Y, axis=1)\n\nCLASSES = list(encoder.classes_)\nprint(classification_report(true_Y, pred_Y, target_names=CLASSES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_confusion_matrix(round(accuracy, 4), CLASSES, pred_Y, true_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.plot(lstm.history['accuracy'], label='LSTM train', linestyle='-', color='b')\nplt.plot(lstm.history['val_accuracy'], label='LSTM validation', linestyle=':', color='b')\nplt.plot(cnn.history['accuracy'], label='CNN train', linestyle='-', color='g')\nplt.plot(cnn.history['val_accuracy'], label='CNN validation', linestyle=':', color='g')\nplt.title('Training and validation accuracy')\nplt.xlabel('Number of epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nplt.plot(lstm.history['loss'], label='LSTM train', linestyle='-', color='b')\nplt.plot(lstm.history['val_loss'], label='LSTM validation', linestyle=':', color='b')\nplt.plot(cnn.history['loss'], label='CNN train', linestyle='-', color='g')\nplt.plot(cnn.history['val_loss'], label='CNN validation', linestyle=':', color='g')\nplt.title('Training and validation loss')\nplt.xlabel('Number of epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}